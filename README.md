## Blog Posts

### [Using large language models (LLMs) to synthesize training data](https://www.amazon.science/blog/using-large-language-models-llms-to-synthesize-training-data)

**Andy Rosenbaum**, Saleh Soltan, Wael Hamza

Amazon Science \| January 2023

## Publications

### Recipes for Sequential Pre-training of Multilingual Encoder and Seq2Seq Models

Saleh Soltan, **Andy Rosenbaum**, Tobias Falke, Qin Lu, Anna Rumshisky, Wael Hamza

Presented at Findings of [ACL 2023](https://2023.aclweb.org) (The 61st Annual Meeting of the Association for Computational Linguistics) and [SustaiNLP 2023](https://sites.google.com/view/sustainlp2023) (Fourth Workshop on Simple and Efficient Natural Language Processing) \| Toronto, ON, Canada \| July 9-14, 2023

[Amazon Science](https://www.amazon.science/publications/recipes-for-sequential-pre-training-of-multilingual-encoder-and-seq2seq-models) \| [arXiv](https://arxiv.org/abs/2306.08756)

### Sampling bias in NLU models: Impact and mitigation

Zefei Li, Anil Ramakrishna, Anna Rumshisky, **Andy Rosenbaum**, Saleh Soltan, Rahul Gupta

Presented at [Interspeech 2023](https://interspeech2023.org) \| Dublin, Ireland \| August 20-24, 2023

[Amazon Science](https://www.amazon.science/publications/sampling-bias-in-nlu-models-impact-and-mitigation)

### PLACES: Prompting Language Models for Social Conversation Synthesis

Maximillian Chen, Alexandros Papangelis, Chenyang Tao, Seokhwan Kim, **Andy Rosenbaum**, Yang Liu, Dilek Hakkani-Tür

Presented at [EACL 2023](https://2023.eacl.org/) (The 17th Conference of the European Chapter
of the Association for Computational Linguistics) \| Dubrovnik, Croatia \| May 2-6, 2023

[ACL Anthology](https://aclanthology.org/2023.findings-eacl.63/) \| [Amazon Science](https://www.amazon.science/publications/places-prompting-language-models-for-social-conversation-synthesis) \| [arXiv](https://arxiv.org/abs/2302.03269)

### Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding

Maximillian Chen, Alexandros Papangelis, Chenyang Tao, **Andy Rosenbaum**, Seokhwan Kim, Yang Liu, Zhou Yu, Dilek Hakkani-Tur

Presented at [SyntheticData4ML @ NeurIPS 2022](https://www.syntheticdata4ml.vanderschaar-lab.com/) \| New Orleans, LA, USA \| December 2, 2022

[Amazon Science](https://www.amazon.science/publications/weakly-supervised-data-augmentation-through-prompting-for-dialogue-understanding) \| [arXiv](https://arxiv.org/abs/2210.14169)

### CLASP: Few-Shot Cross-Lingual Data Augmentation for Semantic Parsing

**Andy Rosenbaum**, Saleh Soltan, Wael Hamza, Amir Saffari, Marco Damonte, Isabel Groves

Presented at [AACL-IJCNLP 2022](https://www.aacl2022.org/) (The 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing) \| Online \| November 20-23, 2022

[ACL Anthology](https://aclanthology.org/2022.aacl-short.56/) \| [Amazon Science](https://www.amazon.science/publications/clasp-few-shot-cross-lingual-data-augmentation-for-semantic-parsing) \| [arXiv](https://arxiv.org/abs/2210.07074)

### LINGUIST: Language Model Instruction Tuning to Generate Annotated Utterances for Intent Classification and Slot Tagging

**Andy Rosenbaum**, Saleh Soltan, Wael Hamza, Yannick Versley, Markus Boese

Presented at [COLING 2022](https://coling2022.org) (The 29th International Conference on Computational Linguistics) \| Gyeongju, Republic of Korea \| October 12-17, 2022.

[ACL Anthology](https://aclanthology.org/2022.coling-1.18/) \| [Amazon Science](https://www.amazon.science/publications/linguist-language-model-instruction-tuning-to-generate-annotated-utterances-for-intent-classification-and-slot-tagging) \| [arXiv](https://arxiv.org/abs/2209.09900)

### AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model

Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith Peris, Stephen Rawls, **Andy Rosenbaum**, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur, Prem Natarajan

August 2022

[Amazon Science Paper](https://www.amazon.science/publications/alexatm-20b-few-shot-learning-using-a-large-scale-multilingual-seq2seq-model) \| [Amazon Science Blog Post](https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning) \| [Amazon Science Code](https://www.amazon.science/code-and-datasets/alexa-teacher-model-alexatm-20b) \| [arXiv](https://arxiv.org/abs/2208.01448)

### Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems

Jack FitzGerald, Shankar Ananthakrishnan, Konstantine Arkoudas, Davide Bernardi, Abhishek Bhagia, Claudio Delli Bovi, Jin Cao, Rakesh Chada, Amit Chauhan, Luoxin Chen, Anurag Dwarakanath, Satyam Dwivedi, Turan Gojayev, Karthik Gopalakrishnan, Thomas Gueudre, Dilek Hakkani-Tur, Wael Hamza, Jonathan Hueser, Kevin Martin Jose, Haidar Khan, Beiye Liu, Jianhua Lu, Alessandro Manzotti, Pradeep Natarajan, Karolina Owczarzak, Gokmen Oz, Enrico Palumbo, Charith Peris, Chandana Satya Prakash, Stephen Rawls, **Andy Rosenbaum**, Anjali Shenoy, Saleh Soltan, Mukund Harakere Sridhar, Liz Tan, Fabian Triefenbach, Pan Wei, Haiyang Yu, Shuai Zheng, Gokhan Tur, Prem Natarajan

Presented at [KDD 2022](https://kdd.org/kdd2022/) (The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining) \| Washington, DC, USA \| August 14-18, 2022

[Amazon Science](https://www.amazon.science/publications/alexa-teacher-model-pretraining-and-distilling-multi-billion-parameter-encoders-for-natural-language-understanding-systems) \| [arXiv](https://arxiv.org/abs/2206.07808)

## Patents

### Active Learning for Lexical Annotation

Alok Ulhas Parlikar, **Andrew Jake Rosenbaum**, Jeffrey Paul Lilly, Jeffrey Penrod Adams 

Abstract: Features are disclosed for active learning to identify the words which are likely to improve the guessing and automatic speech recognition (ASR) after manual annotation. When a speech recognition system needs pronunciations for words, a lexicon is typically used. For unknown words, pronunciation-guessing (G2P) may be included to provide pronunciations in an unattended (e.g., automatic) fashion. However, having manually (e.g., by a human) annotated pronunciations provides better ASR than having automatic pronunciations that may, in some instances, be wrong. The included active learning features help to direct these limited annotation resources.

2014

[Google Patents](https://patents.google.com/patent/US9508341B1/en) \| Patent Number: US 9,508,341 B1

## Education

* M.A. in Computational Linguistics \| Brandeis University \| 2014
* B.S. in Mathematics and Minor in Linguistics \| University of Florida \| 2011

## Licenses and Certifications

* Deep Learning Specialization \| Coursera \| 2018
	* Neural Networks and Deep Learning
	* Structuring Machine Learning Projects
	* Convolutional Neural Networks
	* Sequence Models
	* Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization
* Machine Learning \| Coursera \| 2017
* Trinity College London Certificate in Teaching English to Speakers of Other Languages (TESOL) \| Active Language Institute \| Cádiz, Spain \| 2011

## Languages

* English: Native
* Spanish: Full Working Proficiency
* Esperanto: Full Working Proficiency
* French: Limited Working Proficiency
* Mandarin Chinese: Elementary Proficiency
* Hebrew: Elementary Proficiency

## Sites

* [Google Scholar](https://scholar.google.com/citations?user=r3HxDqAAAAAJ&hl=en)
* [Semantic Scholar](https://www.semanticscholar.org/author/Andrew-Rosenbaum/146177177)
* [Twitter](https://twitter.com/andyjrosenbaum) (@andyjrosenbaum)
* [LinkedIn](https://www.linkedin.com/in/andyjrosenbaum) (@andyjrosenbaum)
